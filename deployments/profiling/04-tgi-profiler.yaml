# TGI GPU Profiler Job
# Profiles HuggingFace TGI inference server with nsys
#
# Prerequisites:
# 1. TGI server must be running: kubectl apply -f ../tgi/01-tgi-llama.yaml
# 2. Wait for TGI to be ready: kubectl logs -f <tgi-pod>
#
# Usage:
# kubectl apply -f 04-tgi-profiler.yaml
# kubectl logs -f job/tgi-profiler -n nim-bench
#
apiVersion: batch/v1
kind: Job
metadata:
  name: tgi-profiler
  namespace: nim-bench
  labels:
    app: tgi-profiler
    framework: tgi
spec:
  backoffLimit: 1
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: tgi-profiler
    spec:
      restartPolicy: Never
      serviceAccountName: nim-bench-sa
      securityContext:
        runAsUser: 0
      containers:
      - name: profiler
        image: docker.io/nvidia/cuda:12.4.0-devel-ubuntu22.04
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "=== TGI GPU Profiler ==="

          # Install dependencies
          apt-get update && apt-get install -y curl > /dev/null 2>&1

          # Install nsys
          dpkg --force-depends -i /tools/nsight-systems-cli-2025.6.1.deb 2>/dev/null || true
          export PATH="/opt/nvidia/nsight-systems-cli/2025.6.1/bin:$PATH"

          # Wait for TGI Llama
          echo "Waiting for TGI Llama service..."
          until curl -s http://tgi-llama-svc:8000/health > /dev/null 2>&1; do
              sleep 5
              echo "  Waiting..."
          done
          echo "TGI Llama is ready!"

          # Warmup
          echo "Running warmup..."
          for i in 1 2 3; do
              curl -s http://tgi-llama-svc:8000/v1/completions \
                  -H "Content-Type: application/json" \
                  -d '{"model":"/models/llama-3-8b-instruct","prompt":"warmup","max_tokens":10}' > /dev/null
          done

          # Profile TGI Llama
          echo ""
          echo "Profiling TGI Llama-3-8B..."
          mkdir -p /results/tgi

          nsys profile \
              --output=/results/tgi/tgi_llama_profile \
              --force-overwrite=true \
              --trace=cuda,nvtx,cudnn,cublas \
              --cuda-memory-usage=true \
              --duration=30 \
              curl -s http://tgi-llama-svc:8000/v1/completions \
                  -H "Content-Type: application/json" \
                  -d '{"model":"/models/llama-3-8b-instruct","prompt":"Write a detailed essay about AI","max_tokens":256}'

          echo ""
          echo "=== TGI Llama Profile Complete ==="
          ls -la /results/tgi/

          # Check for TGI Mistral
          echo ""
          echo "Checking for TGI Mistral service..."
          if curl -s http://tgi-mistral-svc:8000/health > /dev/null 2>&1; then
              echo "TGI Mistral found, profiling..."

              nsys profile \
                  --output=/results/tgi/tgi_mistral_profile \
                  --force-overwrite=true \
                  --trace=cuda,nvtx,cudnn,cublas \
                  --cuda-memory-usage=true \
                  --duration=30 \
                  curl -s http://tgi-mistral-svc:8000/v1/completions \
                      -H "Content-Type: application/json" \
                      -d '{"model":"/models/mistralai--Mistral-7B-Instruct-v0.3","prompt":"Write a detailed essay about AI","max_tokens":256}'

              echo "TGI Mistral profile complete"
          else
              echo "TGI Mistral not running, skipping"
          fi

          echo ""
          echo "=== All TGI Profiles Complete ==="
          ls -la /results/tgi/

          echo "Pod staying alive for 30 min..."
          sleep 1800

        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"

        volumeMounts:
        - name: tools
          mountPath: /tools
          readOnly: true
        - name: results
          mountPath: /results

      volumes:
      - name: tools
        hostPath:
          path: /mnt/coecommonfss/llmcore/2026-NIM-vLLM_LLM/tools
          type: Directory
      - name: results
        hostPath:
          path: /mnt/coecommonfss/llmcore/2026-NIM-vLLM_LLM/results
          type: DirectoryOrCreate

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

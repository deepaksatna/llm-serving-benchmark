# NIM GPU Profiler Job
# Author: Deepak Soni
# Profiles NVIDIA NIM inference server with nsys
#
# Prerequisites:
# 1. NIM server must be running: kubectl apply -f ../nim/01-nim-llama.yaml
# 2. Wait for NIM to be ready: kubectl logs -f <nim-pod>
#
# Usage:
# kubectl apply -f 02-nim-profiler.yaml
# kubectl logs -f job/nim-profiler -n nim-bench
#
apiVersion: batch/v1
kind: Job
metadata:
  name: nim-profiler
  namespace: nim-bench
  labels:
    app: nim-profiler
    framework: nim
spec:
  backoffLimit: 1
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: nim-profiler
    spec:
      restartPolicy: Never
      serviceAccountName: nim-bench-sa
      securityContext:
        runAsUser: 0
      containers:
      - name: profiler
        image: docker.io/nvidia/cuda:12.4.0-devel-ubuntu22.04
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "=== NIM GPU Profiler ==="

          # Install dependencies
          apt-get update && apt-get install -y curl > /dev/null 2>&1

          # Install nsys
          dpkg --force-depends -i /tools/nsight-systems-cli-2025.6.1.deb 2>/dev/null || true
          export PATH="/opt/nvidia/nsight-systems-cli/2025.6.1/bin:$PATH"

          # Wait for NIM Llama
          echo "Waiting for NIM Llama service..."
          until curl -s http://llama3-nim-svc:8000/v1/health/ready > /dev/null 2>&1; do
              sleep 5
              echo "  Waiting..."
          done
          echo "NIM Llama is ready!"

          # Warmup
          echo "Running warmup..."
          for i in 1 2 3; do
              curl -s http://llama3-nim-svc:8000/v1/completions \
                  -H "Content-Type: application/json" \
                  -d '{"model":"meta/llama-3.1-8b-instruct","prompt":"warmup","max_tokens":10}' > /dev/null
          done

          # Profile NIM Llama
          echo ""
          echo "Profiling NIM Llama-3.1-8B..."
          mkdir -p /results/nim

          nsys profile \
              --output=/results/nim/llama3_nim_profile \
              --force-overwrite=true \
              --trace=cuda,nvtx,cudnn,cublas \
              --cuda-memory-usage=true \
              --duration=30 \
              curl -s http://llama3-nim-svc:8000/v1/completions \
                  -H "Content-Type: application/json" \
                  -d '{"model":"meta/llama-3.1-8b-instruct","prompt":"Write a detailed essay about AI","max_tokens":256}'

          echo ""
          echo "=== NIM Llama Profile Complete ==="
          ls -la /results/nim/

          # Wait for NIM Mistral
          echo ""
          echo "Waiting for NIM Mistral service..."
          until curl -s http://mistral-nim-svc:8000/v1/health/ready > /dev/null 2>&1; do
              sleep 5
              echo "  Waiting..."
          done
          echo "NIM Mistral is ready!"

          # Profile NIM Mistral
          echo ""
          echo "Profiling NIM Mistral-7B..."

          nsys profile \
              --output=/results/nim/mistral_nim_profile \
              --force-overwrite=true \
              --trace=cuda,nvtx,cudnn,cublas \
              --cuda-memory-usage=true \
              --duration=30 \
              curl -s http://mistral-nim-svc:8000/v1/completions \
                  -H "Content-Type: application/json" \
                  -d '{"model":"mistralai/mistral-7b-instruct-v03","prompt":"Write a detailed essay about AI","max_tokens":256}'

          echo ""
          echo "=== All NIM Profiles Complete ==="
          ls -la /results/nim/

          # Keep alive for file extraction
          echo "Pod staying alive for 30 min..."
          sleep 1800

        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"

        volumeMounts:
        - name: tools
          mountPath: /tools
          readOnly: true
        - name: results
          mountPath: /results

      volumes:
      - name: tools
        hostPath:
          path: /mnt/coecommonfss/llmcore/2026-NIM-vLLM_LLM/tools
          type: Directory
      - name: results
        hostPath:
          path: /mnt/coecommonfss/llmcore/2026-NIM-vLLM_LLM/results
          type: DirectoryOrCreate

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
